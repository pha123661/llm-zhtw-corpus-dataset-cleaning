{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disks/local/c.4t_4/oscar/zhtw-dataset-cleaning/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coct_en2tw.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10389 examples [00:00, 106104.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpaca_zhtw_formatted.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 104004 examples [00:00, 210543.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idiom.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 111 examples [00:00, 26398.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'conversations', 'meta'],\n",
      "    num_rows: 114504\n",
      "})\n",
      "{'id': 1063099886867503327, 'conversations': [{'role': 'user', 'content': 'Kindly render the English passage below into Traditional Chinese: To meet the challenge posed by the Internet, the World Association of Newspapers met in Madrid last month. Representatives from around the world agreed that if the traditional newspaper industry is to turn the crisis triggered by the Internet into an opportunity for market leadership, it will have to embrace digital technology, expand web-based services, and develop new online profit models.'}, {'role': 'assistant', 'content': '針對網路對傳統報業的衝擊，上月初「世界報業協會」於西班牙馬德里舉行的會議裡，各國與會代表一致同意，惟有更積極擁抱數位科技，努力擴展網路服務，並開發新的線上獲利模式，傳統報業才有可能把網路所帶來的危機，轉化為自己的市場優勢。'}], 'meta': 'None'}\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "for dataset_name in os.listdir(\"instruction_tuning\"):\n",
    "    if not dataset_name.endswith(\".jsonl\"):\n",
    "        continue\n",
    "    print(dataset_name)\n",
    "    datasets.append(load_dataset(\"json\", data_files=f\"instruction_tuning/{dataset_name}\", split='train'))\n",
    "\n",
    "custom_dataset = concatenate_datasets(datasets)\n",
    "custom_dataset = custom_dataset.add_column('meta',  [\"None\"] * len(custom_dataset))\n",
    "print(custom_dataset)\n",
    "print(custom_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 119413 examples [00:09, 12954.63 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 120/120 [00:00<00:00, 138.36ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:05<00:00,  5.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/liswei/en2tw-alignment-sft/commit/4a190e6655d42a4432b6a39a28bc088c2c9ea803', commit_message='Upload dataset', commit_description='', oid='4a190e6655d42a4432b6a39a28bc088c2c9ea803', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from datasets import Dataset\n",
    "import itertools\n",
    "def sort_conv_dict(example):\n",
    "    sorted_conversations = []\n",
    "    for conv in example['conversations']:\n",
    "        dict_ = OrderedDict()\n",
    "        dict_['role'] = conv['role']\n",
    "        dict_['content'] = conv['content']\n",
    "        sorted_conversations.append(dict_)\n",
    "    example['conversations'] = sorted_conversations\n",
    "    return example\n",
    "\n",
    "\n",
    "aya = load_from_disk('instruction_tuning/aya_dataset')\n",
    "\n",
    "# initialize a dataset with content of both datasets\n",
    "def gen():\n",
    "    for i in itertools.chain(custom_dataset, aya):\n",
    "        yield i\n",
    "en2tw_dataset = Dataset.from_generator(gen)\n",
    "en2tw_dataset.push_to_hub(\"en2tw-alignment-sft\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
